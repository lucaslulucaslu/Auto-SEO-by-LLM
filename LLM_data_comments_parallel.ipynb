{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e3cab3aa-d600-4e0c-8b94-5b4064d75a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import TypedDict\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import requests\n",
    "from utilities.llm_wrapper import llm_wrapper_raw\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "import markdown2\n",
    "\n",
    "version = \"2025-03-23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6e5cbba8-781b-48ac-a97c-1f0fba1c87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(\n",
    "    db=os.environ[\"db_name\"],\n",
    "    user=os.environ[\"db_user\"],\n",
    "    passwd=os.environ[\"db_pass\"],\n",
    "    host=os.environ[\"db_host\"],\n",
    "    port=3306,\n",
    "    cursorclass=pymysql.cursors.DictCursor,\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "query = \"\"\"SELECT hd,ca,efa,efb,ranking FROM fp_IPEDS.latest_information\"\"\"\n",
    "cursor.execute(query)\n",
    "row = cursor.fetchone()\n",
    "# connection.commit()\n",
    "hd_year = row[\"hd\"]\n",
    "efa_year = row[\"efa\"]\n",
    "rank_year = row[\"ranking\"]\n",
    "major_year = row[\"ca\"]\n",
    "students_year = row[\"efa\"] + 1\n",
    "age_year = row[\"efb\"] + 1\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c2e0c942-2c1f-4f2c-a63b-71682195898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_admin_data_url = \"https://www.forwardpathway.com/d3v7/dataphp/school_database/ranking_admin_20240923.php?name=\"\n",
    "information_data_url = \"https://www.forwardpathway.com/d3v7/dataphp/school_database/school_information_20240821.php?name=\"\n",
    "majors_data_url = (\n",
    "    \"https://www.forwardpathway.com/d3v7/dataphp/chatbot/degrees.php?name=\"\n",
    ")\n",
    "SAT_data_url = \"https://www.forwardpathway.com/d3v7/dataphp/school_database/score10_20231213.php?name=\"\n",
    "students_data_url = \"https://www.forwardpathway.com/d3v7/dataphp/school_database/student_comp_20240118.php?name=\"\n",
    "age_data_url = \"https://www.forwardpathway.com/d3v7/dataphp/school_database/age_mf_20240118.php?name=\"\n",
    "international_data_url = \"https://www.forwardpathway.com/d3v7/dataphp/school_database/international_students_20240118.php?name=\"\n",
    "crime_data_url = \"https://www.forwardpathway.com/d3v7/dataphp/school_database/crime_yearly_20240324.php?name=\"\n",
    "nearby_data_url = (\n",
    "    \"https://www.forwardpathway.com/d3v7/dataphp/chatbot/school_nearby.php?name=\"\n",
    ")\n",
    "finance_data_url = \"https://www.forwardpathway.com/d3v7/dataphp/school_database/finance_yearly_20240118.php?name=\"\n",
    "salary_data_url = \"https://www.forwardpathway.com/d3v7/dataphp/school_database/staff_salary_20240118.php?name=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41ce61-4ead-4b7f-9e1d-67e268df3ed6",
   "metadata": {},
   "source": [
    "---------------排名录取率--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6f178007-274d-4909-a732-32635e8bf3b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def rank_admin_comments(state):\n",
    "    try:\n",
    "        college_postid = state[\"college_postid\"]\n",
    "        college_unitid = state[\"college_unitid\"]\n",
    "        college_name = state[\"college_name\"]\n",
    "        college_cname = state[\"college_cname\"]\n",
    "        comment_string = state[\"comment_string\"]\n",
    "\n",
    "        if college_postid == \"colleges\":\n",
    "            rank_admin_data = pd.read_json(\n",
    "                rank_admin_data_url + college_postid + \"&ID=\" + str(college_unitid)\n",
    "            )\n",
    "        else:\n",
    "            rank_admin_data = pd.read_json(rank_admin_data_url + str(college_postid))\n",
    "        rank_data = rank_admin_data[[\"year\", \"rank\"]]\n",
    "        admin_data = rank_admin_data.drop(\"rank\", axis=1)\n",
    "        rank_data.set_index(\"year\", inplace=True)\n",
    "        admin_data.set_index(\"year\", inplace=True)\n",
    "        admin_data = admin_data.dropna(axis=0, how=\"all\")\n",
    "        system_prompt = f\"\"\"下面给出的是{college_name}({college_cname})的排名与录取相关数据，数据中'year'为年份，'rank'为USNewws美国大学排名，\\\n",
    "            'rate'为男生录取率，'rate2'为女生录取率，'enroll'为当年入学人数，'deny'为当年拒绝人数，'defer'为当年录取但延迟入学人数，\\\n",
    "                最新年份（{rank_year}）的排名在评论中一定要提及。{comment_string}\"\"\"\n",
    "        user_prompt = f\"排名数据如下：{rank_data.to_json()}\\n\\n录取率数据如下：{admin_data.to_json()}\"\n",
    "        response = llm_wrapper_raw(system_prompt,user_prompt).text.strip().replace(\"\\n\\n\", \"\\n\")\n",
    "        return {\"rank_admin\": response}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in rank_admin_comments: {e}\")\n",
    "        return {\"rank_admin\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e6c18-3867-4fa8-b565-7a4e52544f84",
   "metadata": {},
   "source": [
    "------------------------基础数据-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2e625173-1046-4599-bdc4-2af9bd4948b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def information_comments(state):\n",
    "    try:\n",
    "        college_postid = state[\"college_postid\"]\n",
    "        college_unitid = state[\"college_unitid\"]\n",
    "        college_name = state[\"college_name\"]\n",
    "        college_cname = state[\"college_cname\"]\n",
    "        comment_string = state[\"comment_string\"]\n",
    "        if college_postid == \"colleges\":\n",
    "            reqeust_response = requests.get(\n",
    "                information_data_url + college_postid + \"&ID=\" + str(college_unitid)\n",
    "            )\n",
    "        else:\n",
    "            reqeust_response = requests.get(information_data_url + str(college_postid))\n",
    "        information_data = json.loads(reqeust_response.content)\n",
    "        system_prompt = f\"\"\"下面给出的是{college_name}({college_cname})的相关数据，数据中'type'为学校类型，1为公立，2为私立，'tuition'为学费数据，其中'year'为年份，\\\n",
    "            'tuition_in_under'为州内本科学费，'tuition_out_under'为外州本科生学费，'tuition_in_grad'为州内研究生学费，'tuition_out_grad'为外州研究生学费。\\\n",
    "                'students'为学生人数数据，其中'year'为年份，'students_under'为该校本科生人数，'students_grad'为该校研究生人数。'm2w'为男女比例数据，其中'year'为年份，\\\n",
    "                    'm2w_men_under'为本科生男生占比，'m2w_women_under'为本科生女生占比，'m2w_men_grad'为研究生男生占比，'m2w_women_grad'为研究生女生占比。\\\n",
    "                        'graduation'为毕业率数据，其中'year'为年份，'graduation_100_under'为本科生按时毕业率，'graduation_150_under'为本科生150%时间毕业率。\\\n",
    "                            'retention'为学生保有率数据，其中'year'为年份，'retention_under'为学生保有率，'s2f'为学生教授比，其中'year'为年份，'s2f_under'为学生教授比。\\\n",
    "                                选取其中最重要或者变化比较大3-4个数据做讨论，不一定要所有点都提到。{comment_string}\"\"\"\n",
    "        user_prompt = f\"数据如下：{information_data}\"\n",
    "        response = llm_wrapper_raw(system_prompt, user_prompt).text.strip().replace(\"\\n\\n\", \"\\n\")\n",
    "        return {\"information\": response}\n",
    "    except:\n",
    "        return {\"information\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582a8789-aa16-402f-81d0-778ff40a6712",
   "metadata": {},
   "source": [
    "-----------------------------专业数据---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2b9b7389-40b3-46bb-b04f-226ab2914890",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def majors_comments(state):\n",
    "    try:\n",
    "        college_postid = state[\"college_postid\"]\n",
    "        college_unitid = state[\"college_unitid\"]\n",
    "        college_name = state[\"college_name\"]\n",
    "        college_cname = state[\"college_cname\"]\n",
    "        comment_string = state[\"comment_string\"]\n",
    "        if college_postid == \"colleges\":\n",
    "            majors_data = pd.read_json(\n",
    "                majors_data_url + college_postid + \"&ID=\" + str(college_unitid),\n",
    "                orient=\"index\",\n",
    "                convert_axes=False,\n",
    "            ).reset_index(drop=True)\n",
    "        else:\n",
    "            majors_data = pd.read_json(\n",
    "                majors_data_url + str(college_postid),\n",
    "                orient=\"index\",\n",
    "                convert_axes=False,\n",
    "            ).reset_index(drop=True)\n",
    "        majors_data = majors_data.iloc[\n",
    "            majors_data[[\"bachelor\", \"master\", \"doctorate\"]]\n",
    "            .sum(axis=1)\n",
    "            .sort_values(ascending=False)\n",
    "            .index\n",
    "        ]\n",
    "        system_prompt = f\"\"\"下面给出的是{major_year}年{college_name}({college_cname})的专业毕业人数相关数据，数据中'associate'为副学士学位，'bachelor'为本科学位，\\\n",
    "            'master'为硕士学位，'doctorate'为博士学位，'associate_i'为副学士学位国际留学生，'bachelor_i'为本科学位国际留学生，\\\n",
    "                'master_i'为硕士学位国际留学生，'doctorate_i'为博士学位国际留学生，学位后面的数字为毕业年份，该栏对应数字为该专业该年份毕业的人数，\\\n",
    "                    从数据中找几个重要的或者有特点的类别分析，{comment_string}\"\"\"\n",
    "        user_prompt = f\"数据如下：{majors_data.to_json()}\"\n",
    "        response = llm_wrapper_raw(system_prompt, user_prompt).text.strip().replace(\"\\n\\n\", \"\\n\")\n",
    "        return {\"majors\": response}\n",
    "    except:\n",
    "        return {\"majors\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2cdcd7-1226-4cb8-8091-b0d4bf59e921",
   "metadata": {},
   "source": [
    "----------------------------------SAT/ACT--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cd5449fc-d664-429b-9ea4-faab1a6efc17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def SAT_comments(state):\n",
    "    try:\n",
    "        college_postid = state[\"college_postid\"]\n",
    "        college_unitid = state[\"college_unitid\"]\n",
    "        college_name = state[\"college_name\"]\n",
    "        college_cname = state[\"college_cname\"]\n",
    "        comment_string = state[\"comment_string\"]\n",
    "        if college_postid == \"colleges\":\n",
    "            SAT_data = pd.read_json(\n",
    "                SAT_data_url + college_postid + \"&ID=\" + str(college_unitid)\n",
    "            )\n",
    "        else:\n",
    "            SAT_data = pd.read_json(SAT_data_url + str(college_postid))\n",
    "        system_prompt = f\"\"\"下面给出的是{college_name}({college_cname})的相关数据，第一层为不同年份的数据，下面分为per和score部分，\\\n",
    "            其中per部分包含SAT和ACT成绩提交的人数（'number'）和占录取人数的比例（'per'），score部分包含'SATR'为SAT阅读成绩，'SATM'为SAT数学成绩，\\\n",
    "                'ACTC'为ACT综合成绩，'ACTE'为ACT英语成绩，'ACTM'为ACT数学成绩，'start'是录取学生中25%该项成绩，'end'是录取学生总75%该项成绩。{comment_string}\"\"\"\n",
    "        user_prompt = f\"数据如下：{SAT_data.to_json()}\"\n",
    "        response = llm_wrapper_raw(system_prompt, user_prompt).text.strip().replace(\"\\n\\n\", \"\\n\")\n",
    "        return {\"SAT_ACT\": response}\n",
    "    except:\n",
    "        return {\"SAT_ACT\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c95f70-0401-4ba1-ba5a-587cd8f1b504",
   "metadata": {},
   "source": [
    "------------------------学生组成---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5b50aac7-502c-41e3-8589-e7a76eeb01f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def students_comments(state):\n",
    "    try:\n",
    "        college_postid = state[\"college_postid\"]\n",
    "        college_unitid = state[\"college_unitid\"]\n",
    "        college_name = state[\"college_name\"]\n",
    "        college_cname = state[\"college_cname\"]\n",
    "        comment_string = state[\"comment_string\"]\n",
    "        if college_postid == \"colleges\":\n",
    "            students_data = pd.read_json(\n",
    "                students_data_url + college_postid + \"&ID=\" + str(college_unitid)\n",
    "            )\n",
    "        else:\n",
    "            students_data = pd.read_json(students_data_url + str(college_postid))\n",
    "\n",
    "        system_prompt = f\"\"\"下面给出的是{students_year}年{college_name}({college_cname})的学生组成相关数据，数据中'name'是学生分类，有以下几种：\\\n",
    "            'uf'是本科新生，'uj'是本科老生，'ut'是本科转学生，'gr'是研究生，'value'是该类学生的人数，'ratioM'是该类学生男生占比，'ratioW'是该类学生女生占比，\\\n",
    "                第一项的'year'是数据的年份，subs是该类学生的细分人种，有'wh'是白人，'as'是亚裔，'la'是拉丁裔，'pa'是太平洋岛国及其他，'af'是非裔，'nr'是留学生，\\\n",
    "                    学生人数代码依旧为'value'，男女比例代码也与上一层相同，在回答中请不要提及各个分类的代码，仅使用真实名称，{comment_string}\"\"\"\n",
    "        user_prompt = f\"数据如下：{students_data.to_json()}\"\n",
    "        response = llm_wrapper_raw(system_prompt, user_prompt).text.strip().replace(\"\\n\\n\", \"\\n\")\n",
    "        return {\"stduents_comp\": response}\n",
    "    except:\n",
    "        return {\"stduents_comp\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f3b82-987c-4d25-afa1-03a2a8a52ba2",
   "metadata": {},
   "source": [
    "-------------------学生年龄分布--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d8612255-bb31-4ec7-b41a-d3fe7f524c58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def age_comments(state):\n",
    "    try:\n",
    "        college_postid = state[\"college_postid\"]\n",
    "        college_unitid = state[\"college_unitid\"]\n",
    "        college_name = state[\"college_name\"]\n",
    "        college_cname = state[\"college_cname\"]\n",
    "        comment_string = state[\"comment_string\"]\n",
    "        if college_postid == \"colleges\":\n",
    "            age_data = pd.read_json(\n",
    "                age_data_url + college_postid + \"&ID=\" + str(college_unitid)\n",
    "            )\n",
    "        else:\n",
    "            age_data = pd.read_json(age_data_url + str(college_postid))\n",
    "        age_data[[\"underf\", \"gradf\", \"totalf\"]] = -age_data[\n",
    "            [\"underf\", \"gradf\", \"totalf\"]\n",
    "        ]\n",
    "        system_prompt = f\"\"\"下面给出的是{age_year}年{college_name}({college_cname})的学生年龄组成相关数据，数据中'cat'为不同年龄段的分类，\\\n",
    "            'underm'是本科男生，'underf'是本科女生，'gradm'是研究生男生，'gradf'是研究生女生，'totalm'是所有学生的那声，'totalf'是所有学生的女生，{comment_string}\"\"\"\n",
    "        user_prompt = f\"数据如下：{age_data.to_json()}\"\n",
    "        response = llm_wrapper_raw(system_prompt, user_prompt).text.strip().replace(\"\\n\\n\", \"\\n\")\n",
    "        return {\"age\": response}\n",
    "    except:\n",
    "        return {\"age\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b1d37-76c0-4a04-8b57-aba32843bf32",
   "metadata": {},
   "source": [
    "-------------------国际学生数量-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "85e11aef-fd06-4d97-ba56-12e882d9f2a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def international_comments(state):\n",
    "    try:\n",
    "        college_postid = state[\"college_postid\"]\n",
    "        college_unitid = state[\"college_unitid\"]\n",
    "        college_name = state[\"college_name\"]\n",
    "        college_cname = state[\"college_cname\"]\n",
    "        comment_string = state[\"comment_string\"]\n",
    "        if college_postid == \"colleges\":\n",
    "            international_data = pd.read_json(\n",
    "                international_data_url + college_postid + \"&ID=\" + str(college_unitid)\n",
    "            )\n",
    "        else:\n",
    "            international_data = pd.read_json(\n",
    "                international_data_url + str(college_postid)\n",
    "            )\n",
    "        system_prompt = f\"\"\"下面给出的是{college_name}({college_cname})的国际留学生相关数据，数据中'year'为数据年份，其中'undertotal'为该校本科总学生数量，\\\n",
    "            'under'为留学生数量，'underper'为本科学生中留学生占比，'gradtotal'为该校研究生总学生数量，'grad'为研究生中留学生数量，\\\n",
    "                'gradper'为研究生中留学生占比，{comment_string}\"\"\"\n",
    "        user_prompt = f\"数据如下：{international_data.to_json()}\"\n",
    "        response = llm_wrapper_raw(system_prompt, user_prompt).text.strip().replace(\"\\n\\n\", \"\\n\")\n",
    "        return {\"international\": response}\n",
    "    except:\n",
    "        return {\"international\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb075df-97d6-4f06-b4ad-be35b839b5c9",
   "metadata": {},
   "source": [
    "-------------------------犯罪率---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "568cb900-6ec2-4ac5-b281-511a03ac9adb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def crime_comments(state):\n",
    "    try:\n",
    "        college_postid = state[\"college_postid\"]\n",
    "        college_unitid = state[\"college_unitid\"]\n",
    "        college_name = state[\"college_name\"]\n",
    "        college_cname = state[\"college_cname\"]\n",
    "        comment_string = state[\"comment_string\"]\n",
    "        if college_postid == \"colleges\":\n",
    "            crime_data = pd.read_json(\n",
    "                crime_data_url + college_postid + \"&ID=\" + str(college_unitid)\n",
    "            )\n",
    "        else:\n",
    "            crime_data = pd.read_json(crime_data_url + str(college_postid))\n",
    "        crime_dict = {\n",
    "            \"WA\": \"持枪逮捕\",\n",
    "            \"DA\": \"毒品逮捕\",\n",
    "            \"LA\": \"酗酒逮捕\",\n",
    "            \"WD\": \"持枪记过\",\n",
    "            \"DD\": \"毒品记过\",\n",
    "            \"LD\": \"酗酒记过\",\n",
    "            \"DoV\": \"家暴\",\n",
    "            \"DaV\": \"约会犯罪\",\n",
    "            \"S\": \"跟踪\",\n",
    "            \"M\": \"谋杀\",\n",
    "            \"NM\": \"过失杀人\",\n",
    "            \"Ra\": \"强奸\",\n",
    "            \"F\": \"性扰\",\n",
    "            \"I\": \"乱伦\",\n",
    "            \"Ro\": \"抢劫\",\n",
    "            \"AA\": \"袭击\",\n",
    "            \"B\": \"盗窃\",\n",
    "            \"VT\": \"偷车\",\n",
    "            \"A\": \"纵火\",\n",
    "            \"RHF\": \"宿舍火灾\",\n",
    "        }\n",
    "        system_prompt = f\"\"\"下面给出的是{college_name}({college_cname})的校园记过、犯罪相关数据，数据中'year'为每组数据的年份，'crime_total'可以忽略，\\\n",
    "            'avg1000'为每1000学生记过、犯罪率，'subdata'为细分项，项目名称代码转换为真实名称如下表：{crime_dict}，其中'numberR'为数量，忽略'number'项，\\\n",
    "                在回答最后附上犯罪率的计算方法链接：'[https://www.forwardpathway.com/33447](https://www.forwardpathway.com/33447)'，{comment_string}\"\"\"\n",
    "        user_prompt = f\"数据如下：{crime_data.to_json()}\"\n",
    "        response = llm_wrapper_raw(system_prompt, user_prompt).text.strip()\n",
    "        response = markdown2.markdown(response)\n",
    "        return {\"crime\": response}\n",
    "    except:\n",
    "        return {\"crime\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c867ec-4fe4-49ba-9275-55a4c6e3fb55",
   "metadata": {},
   "source": [
    "------------------------周边名校------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "260082e4-ac6d-4485-9ecd-1b814b024d13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def nearby_comments(state):\n",
    "    try:\n",
    "        college_postid = state[\"college_postid\"]\n",
    "        college_unitid = state[\"college_unitid\"]\n",
    "        college_name = state[\"college_name\"]\n",
    "        college_cname = state[\"college_cname\"]\n",
    "        language_string = state[\"language_string\"]\n",
    "        fpus = state[\"fpus\"]\n",
    "        nearby_url = nearby_data_url\n",
    "        if college_postid == \"colleges\":\n",
    "            nearby_url = nearby_url + college_postid + \"&ID=\" + str(college_unitid)\n",
    "        else:\n",
    "            nearby_url = nearby_url + str(college_postid)\n",
    "        if fpus == \"fpus\":\n",
    "            nearby_url = nearby_url + \"&v=fpus\"\n",
    "            url_base = \"https://www.forwardpathway.us\"\n",
    "        else:\n",
    "            url_base = \"https://www.forwardpathway.com\"\n",
    "\n",
    "        nearby_data = pd.read_json(nearby_url)\n",
    "        nearby_data = (\n",
    "            nearby_data.sort_values(\"distance\").drop(columns=[\"distance\"]).head(20)\n",
    "        )\n",
    "        system_prompt = f\"\"\"下面给出的是{college_name}({college_cname})的周边名校相关数据，数据中'name'为英文名，'cname'为中文名，\\\n",
    "            'rank'为{str(rank_year)}USNews排名，'type'为学校类型，1为综合大学，2为文理学院，postid为学校url链接路径。\\\n",
    "                对该校周边名校中挑选重点做一个简单的介绍，提到的学校名上面需要需要超链接，链接名为学校名称，链接url为'{url_base}/postid'，\\\n",
    "                    其中postid部分请替换为数据中的postid。{language_string}\"\"\"\n",
    "        user_prompt = f\"数据如下：{nearby_data.to_json()}\"\n",
    "        response = llm_wrapper_raw(system_prompt, user_prompt).text.strip()\n",
    "        response = markdown2.markdown(response)\n",
    "        return {\"nearby\": response}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in nearby_comments: {e}\")\n",
    "        return {\"nearby\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71126a6-aa87-44cd-a97f-58fe1bb3de3d",
   "metadata": {},
   "source": [
    "---------------------------财政收入、支出----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "46398358-0150-41c0-ae9c-a9a791dfd3d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def finance_convert(x, type):\n",
    "    for item in x:\n",
    "        if item[\"type\"] == type:\n",
    "            return item[\"num\"]\n",
    "    return\n",
    "\n",
    "\n",
    "def finance_comments(state):\n",
    "    try:\n",
    "        college_postid = state[\"college_postid\"]\n",
    "        college_unitid = state[\"college_unitid\"]\n",
    "        college_name = state[\"college_name\"]\n",
    "        college_cname = state[\"college_cname\"]\n",
    "        comment_string = state[\"comment_string\"]\n",
    "        if college_postid == \"colleges\":\n",
    "            data = pd.read_json(\n",
    "                finance_data_url + college_postid + \"&ID=\" + str(college_unitid)\n",
    "            )\n",
    "        else:\n",
    "            data = pd.read_json(finance_data_url + str(college_postid))\n",
    "        revenue_array = [\n",
    "            \"Year\",\n",
    "            \"学杂费\",\n",
    "            \"联邦政府拨款\",\n",
    "            \"州政府拨款\",\n",
    "            \"地方政府拨款\",\n",
    "            \"政府补助、外包合同\",\n",
    "            \"私人礼物、赠款、合同\",\n",
    "            \"投资回报\",\n",
    "            \"资产收入\",\n",
    "            \"教育活动收入\",\n",
    "            \"其他收入\",\n",
    "            \"附属企业收入\",\n",
    "            \"附属医院收入\",\n",
    "            \"独立活动收入\",\n",
    "        ]\n",
    "        revenue_df = pd.DataFrame(columns=revenue_array)\n",
    "        revenue_df[\"Year\"] = data[\"year\"]\n",
    "        for x in revenue_array[1:]:\n",
    "            revenue_df[x] = data[\"revenue\"].apply(finance_convert, type=x)\n",
    "        revenue_df.dropna(axis=1, inplace=True)\n",
    "        revenue_df[\"总收入\"] = revenue_df.sum(axis=1)\n",
    "        expense_array = [\n",
    "            \"Year\",\n",
    "            \"教学活动\",\n",
    "            \"研究活动\",\n",
    "            \"公共服务\",\n",
    "            \"学术支持\",\n",
    "            \"学生服务\",\n",
    "            \"学院支持\",\n",
    "            \"奖学金支出\",\n",
    "            \"其他支出\",\n",
    "            \"附属企业支出\",\n",
    "            \"附属医院支出\",\n",
    "            \"独立活动支出\",\n",
    "        ]\n",
    "        expense_df = pd.DataFrame(columns=expense_array)\n",
    "        expense_df[\"Year\"] = data[\"year\"]\n",
    "        for x in expense_array[1:]:\n",
    "            expense_df[x] = data[\"expense\"].apply(finance_convert, type=x)\n",
    "        expense_df.dropna(axis=1, inplace=True)\n",
    "        expense_df[\"总支出\"] = expense_df.sum(axis=1)\n",
    "        system_prompt = f\"\"\"下面给出的是{college_name}({college_cname})的财政收支数据，数据单位为美元，财政收入数据需要特别注意投资回报为负数的年份，\\\n",
    "            计算收入占比可以用当年分类数值除以总收入数值，财政支出数据计算支出占比可以用当年的分类数值除以总支出数值，财政数字尽量使用万，亿等单位，{comment_string}\"\"\"\n",
    "        user_prompt = f\"财政收入数据如下：{revenue_df.to_json()}\\n\\n财政支出数据如下：{expense_df.to_json()}\"\n",
    "        response = llm_wrapper_raw(system_prompt, user_prompt).text.strip().replace(\"\\n\\n\", \"\\n\")\n",
    "        return {\"finanace\": response}\n",
    "    except:\n",
    "        return {\"finanace\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdd7c2-92a9-439f-9607-488acd49b5c1",
   "metadata": {},
   "source": [
    "--------------------------------教职工工资-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "17738d87-0b57-44c0-8ba6-f0047a862d6e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def salary_convert(x, typeR, typeS):\n",
    "    for item in x:\n",
    "        if item[\"r\"] == typeR:\n",
    "            return item[typeS]\n",
    "    return\n",
    "\n",
    "\n",
    "def salary_comments(state):\n",
    "    try:\n",
    "        college_postid = state[\"college_postid\"]\n",
    "        college_unitid = state[\"college_unitid\"]\n",
    "        college_name = state[\"college_name\"]\n",
    "        college_cname = state[\"college_cname\"]\n",
    "        comment_string = state[\"comment_string\"]\n",
    "        salary_dict = {\n",
    "            1: \"教授\",\n",
    "            2: \"副教授\",\n",
    "            3: \"助理教授\",\n",
    "            4: \"讲师 (Instructor)\",\n",
    "            5: \"讲师 (Lecturer)\",\n",
    "            6: \"无职称\",\n",
    "            7: \"所有教职工\",\n",
    "        }\n",
    "        if college_postid == \"colleges\":\n",
    "            data = pd.read_json(\n",
    "                salary_data_url + college_postid + \"&ID=\" + str(college_unitid)\n",
    "            )\n",
    "        else:\n",
    "            data = pd.read_json(salary_data_url + str(college_postid))\n",
    "        df = pd.DataFrame()\n",
    "        df[\"Year\"] = data.year\n",
    "        for key, item in salary_dict.items():\n",
    "            df[item + \"(平均)\"] = data[\"data\"].apply(\n",
    "                salary_convert, typeR=key, typeS=\"t\"\n",
    "            )\n",
    "            df[item + \"（男）\"] = data[\"data\"].apply(\n",
    "                salary_convert, typeR=key, typeS=\"m\"\n",
    "            )\n",
    "            df[item + \"（女）\"] = data[\"data\"].apply(\n",
    "                salary_convert, typeR=key, typeS=\"w\"\n",
    "            )\n",
    "        df.dropna(axis=1, inplace=True)\n",
    "        system_prompt = f\"\"\"下面给出的是{college_name}({college_cname})的教职工工资数据，数据单位为美元，数据中'year'为年份，{comment_string}\"\"\"\n",
    "        user_prompt = f\"数据如下：{df.to_json()}\"\n",
    "        response = llm_wrapper_raw(system_prompt, user_prompt).text.strip().replace(\"\\n\\n\", \"\\n\")\n",
    "        return {\"salary\": response}\n",
    "    except:\n",
    "        return {\"salary\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb81192-c83b-4ae9-bc2d-44cb626248de",
   "metadata": {},
   "source": [
    "--------------------------------简介-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d3461959-06ba-48b2-b050-bce50c9ead81",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def intro_info(state):\n",
    "    college_name = state[\"college_name\"]\n",
    "    college_cname = state[\"college_cname\"]\n",
    "    language_string = state[\"language_string\"]\n",
    "    system_prompt = f\"\"\"对下面给出的这所美国大学的中文名和英文名，结合你了解的情况，对该学校做一个500字左右最多两个段落的简单综合介绍，\\\n",
    "        可以从下面的几个方面随机选取3-4个方面介绍学校，可以选择的方面如下：特点、校训、人文环境、校园文化、学校设施、师资配置、地理位置、毕业生就业、\\\n",
    "            专业、体育运动、学生生活、杰出校友、最近的热点新闻等等，内容不可以虚构必须是事实，内容尽量不要提及任何负面信息。{language_string}\"\"\"\n",
    "    user_prompt = f\"该美国大学中文名如下：{college_cname}，英文名如下：{college_name}\"\n",
    "    response = llm_wrapper_raw(system_prompt, user_prompt).text.strip().replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "    return {\"intro\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8191e-7fae-4bcd-b1f1-c9584603503a",
   "metadata": {},
   "source": [
    "--------------------------------get colleges list----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d4948a35-85fa-4f3b-b26a-192a55beb609",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_colleges_list(post_type=\"postid\"):\n",
    "    connection = pymysql.connect(\n",
    "        db=os.environ[\"db_name\"],\n",
    "        user=os.environ[\"db_user\"],\n",
    "        passwd=os.environ[\"db_pass\"],\n",
    "        host=os.environ[\"db_host\"],\n",
    "        port=3306,\n",
    "        cursorclass=pymysql.cursors.DictCursor,\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    if post_type == \"unitid\":\n",
    "        query = \"\"\"SELECT \"colleges\" as postid, tfp.postid as fpus_id,t1.UNITID as unitid,t1.INSTNM as name,t3.translation as cname,NULL as type FROM fp_IPEDS.hd{} t1\n",
    "JOIN fpus_colleges.transform tfp ON tfp.unitid=t1.UNITID\n",
    "JOIN fp_IPEDS.EFA t2 ON t2.UNITID=t1.UNITID AND t2.EFALEVEL=1 AND t2.Year={} AND t2.EFTOTLT>=500\n",
    "LEFT JOIN fp_IPEDS.inst_trans t3 ON t3.UNITID=t1.UNITID\n",
    "WHERE t1.UNITID NOT IN (SELECT unitid FROM fp_ranking.colleges) AND t1.UNITID NOT IN (SELECT unitid FROM fp_chatGPT.data_comments_en WHERE version=\"{}\")\"\"\".format(\n",
    "            hd_year, efa_year, version\n",
    "        )\n",
    "    else:\n",
    "        query = \"\"\"SELECT t1.postid,t2.postid as fpus_id,t1.unitid,t1.name,t1.cname,t1.type FROM fp_ranking.colleges t1 LEFT JOIN fpus_colleges.transform t2 ON t2.unitid=t1.unitid WHERE t1.type IN (1,2)\"\"\"\n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    colleges = pd.DataFrame(\n",
    "        columns=[\"postid\", \"fpus_id\", \"unitid\", \"name\", \"cname\", \"type\"]\n",
    "    )\n",
    "    for row in rows:\n",
    "        colleges = pd.concat([colleges, pd.DataFrame([row])])\n",
    "    colleges = colleges.reset_index(drop=True)\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    return colleges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5d185cf5-d8df-4198-9f4e-063bb0999462",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def initial_college_info_cn(state):\n",
    "    index = state[\"index\"]\n",
    "    college_postid = colleges.iloc[index][\"postid\"]\n",
    "    college_fpusid = colleges.iloc[index][\"fpus_id\"]\n",
    "    college_unitid = colleges.iloc[index][\"unitid\"]\n",
    "    college_name = colleges.iloc[index][\"name\"]\n",
    "    college_cname = colleges.iloc[index][\"cname\"]\n",
    "    comment_string = \"\"\"对该数据做出一段400字左右文字的简短的评论，输出结果尽量不要分段，也不要使用标题。可以结合你知道的其他关于该类数据的信息一起评论，\\\n",
    "        可以结合真实的新闻时事信息，但不要捏造信息，遇到比率等需要转换成百分比数。直接给出评论内容，在回答中不要提及数据项的代号，只使用各项数据的真实名称。\\\n",
    "            数据中有数值的都为已经发布的实际数据，数据中不含任何预计数据，未发布数据的为'NaN'或者空位，这部分可以忽略。如果对数据的多个年份进行评论，请注意年份的顺序，\\\n",
    "                不能出现年份混淆的情况。数据中有年份的地方请注意年份的顺序，不能出现年份混淆的情况。\"\"\"\n",
    "    return {\n",
    "        \"college_postid\": college_postid,\n",
    "        \"college_fpusid\": college_fpusid,\n",
    "        \"college_unitid\": college_unitid,\n",
    "        \"college_name\": college_name,\n",
    "        \"college_cname\": college_cname,\n",
    "        \"comment_string\": comment_string,\n",
    "        \"language_string\": \"\",\n",
    "        \"fpus\": None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ccb918b5-b202-46f9-b796-ef0a09469052",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def initial_college_info_en(state):\n",
    "    index = state[\"index\"]\n",
    "    college_postid = colleges.iloc[index][\"postid\"]\n",
    "    college_fpusid = colleges.iloc[index][\"fpus_id\"]\n",
    "    college_unitid = colleges.iloc[index][\"unitid\"]\n",
    "    college_name = colleges.iloc[index][\"name\"]\n",
    "    college_cname = colleges.iloc[index][\"cname\"]\n",
    "    language_string = (\n",
    "        \"最终输出结果必须是英文，the final output should in purely English language.\"\n",
    "    )\n",
    "    comment_string = f\"\"\"对该数据做出一段400字左右文字的简短的评论，输出结果尽量不要分段，也不要使用标题。可以结合你知道的其他关于该类数据的信息一起评论，\\\n",
    "        可以结合真实的新闻时事信息，但不要捏造信息，遇到比率等需要转换成百分比数。直接给出评论内容，在回答中不要提及数据项的代号，只使用各项数据的真实名称。\\\n",
    "            数据中有数值的都为已经发布的实际数据，数据中不含任何预计数据，未发布数据的为'NaN'或者空位，这部分可以忽略。{language_string}\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"college_postid\": college_postid,\n",
    "        \"college_fpusid\": college_fpusid,\n",
    "        \"college_unitid\": college_unitid,\n",
    "        \"college_name\": college_name,\n",
    "        \"college_cname\": college_cname,\n",
    "        \"comment_string\": comment_string,\n",
    "        \"language_string\": language_string,\n",
    "        \"fpus\": \"fpus\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9305e84a-06b7-4092-aa56-a598ccb63e13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_cn(state):\n",
    "    connection = pymysql.connect(\n",
    "        db=os.environ[\"db_name\"],\n",
    "        user=os.environ[\"db_user\"],\n",
    "        passwd=os.environ[\"db_pass\"],\n",
    "        host=os.environ[\"db_host\"],\n",
    "        port=3306,\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    update_sections = [\n",
    "        \"rank_admin\",\n",
    "        \"information\",\n",
    "        \"majors\",\n",
    "        \"SAT_ACT\",\n",
    "        \"stduents_comp\",\n",
    "        \"age\",\n",
    "        \"international\",\n",
    "        \"crime\",\n",
    "        \"nearby\",\n",
    "        \"finanace\",\n",
    "        \"salary\",\n",
    "        \"intro\",\n",
    "    ]\n",
    "    postid = state[\"college_postid\"]\n",
    "    unitid = state[\"college_unitid\"]\n",
    "    for update_key in update_sections:\n",
    "        query = \"INSERT INTO fp_chatGPT.data_comments (postid,unitid,type,comment,version) VALUES (%s,%s,%s,%s,%s)\"\n",
    "        update_content = state[update_key]\n",
    "        if not update_content:\n",
    "            continue\n",
    "        if postid == \"colleges\":\n",
    "            cursor.execute(\n",
    "                query,\n",
    "                (\n",
    "                    37341,\n",
    "                    unitid,\n",
    "                    update_key,\n",
    "                    update_content,\n",
    "                    version,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                query,\n",
    "                (\n",
    "                    postid,\n",
    "                    unitid,\n",
    "                    update_key,\n",
    "                    update_content,\n",
    "                    version,\n",
    "                ),\n",
    "            )\n",
    "            utc_time_now = datetime.datetime.now(datetime.UTC).strftime(\n",
    "                \"%Y-%m-%d %H:%M:%S\"\n",
    "            )\n",
    "            time_now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            query = \"UPDATE fp_forwardpathway.`wp_mmcp_posts` SET `post_modified`=%s,`post_modified_gmt`=%s WHERE `ID`=%s\"\n",
    "            cursor.execute(query, (time_now, utc_time_now, postid))\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a5e31baa-1b6b-48d8-93d9-09d6d2c84405",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_en(state):\n",
    "    connection = pymysql.connect(\n",
    "        db=os.environ[\"db_name\"],\n",
    "        user=os.environ[\"db_user\"],\n",
    "        passwd=os.environ[\"db_pass\"],\n",
    "        host=os.environ[\"db_host\"],\n",
    "        port=3306,\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    update_sections = [\n",
    "        \"rank_admin\",\n",
    "        \"information\",\n",
    "        \"majors\",\n",
    "        \"SAT_ACT\",\n",
    "        \"stduents_comp\",\n",
    "        \"age\",\n",
    "        \"international\",\n",
    "        \"crime\",\n",
    "        \"nearby\",\n",
    "        \"intro\",\n",
    "    ]\n",
    "    fpusid = state[\"college_fpusid\"]\n",
    "    unitid = state[\"college_unitid\"]\n",
    "    for update_key in update_sections:\n",
    "        query = \"INSERT INTO fp_chatGPT.data_comments_en (postid,unitid,type,comment,version) VALUES (%s,%s,%s,%s,%s)\"\n",
    "        update_content = state[update_key]\n",
    "        if not update_content:\n",
    "            continue\n",
    "        cursor.execute(\n",
    "            query,\n",
    "            (\n",
    "                fpusid,\n",
    "                unitid,\n",
    "                update_key,\n",
    "                update_content,\n",
    "                version,\n",
    "            ),\n",
    "        )\n",
    "        utc_time_now = datetime.datetime.now(datetime.UTC).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        time_now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        query = \"UPDATE fpus_wordpress.`wp_posts` SET `post_modified`=%s,`post_modified_gmt`=%s WHERE `ID`=%s\"\n",
    "        cursor.execute(query, (time_now, utc_time_now, fpusid))\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0869cd7e-4446-4ce5-be2f-8e3a99f203a7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class cnState(TypedDict):\n",
    "    index: int\n",
    "    comment_string: str\n",
    "    language_string: str\n",
    "    college_postid: str\n",
    "    college_fpusid: str\n",
    "    college_unitid: str\n",
    "    college_name: str\n",
    "    college_cname: str\n",
    "    fpus: str\n",
    "    rank_admin: str\n",
    "    information: str\n",
    "    majors: str\n",
    "    SAT_ACT: str\n",
    "    stduents_comp: str\n",
    "    age: str\n",
    "    international: str\n",
    "    crime: str\n",
    "    nearby: str\n",
    "    finanace: str\n",
    "    salary: str\n",
    "    intro: str\n",
    "\n",
    "\n",
    "cn_flow = StateGraph(cnState)\n",
    "cn_flow.add_node(\"initial_node\", initial_college_info_cn)\n",
    "cn_flow.add_node(\"rank_admin_node\", rank_admin_comments)\n",
    "cn_flow.add_node(\"information_node\", information_comments)\n",
    "cn_flow.add_node(\"majors_node\", majors_comments)\n",
    "cn_flow.add_node(\"SAT_node\", SAT_comments)\n",
    "cn_flow.add_node(\"students_node\", students_comments)\n",
    "cn_flow.add_node(\"age_node\", age_comments)\n",
    "cn_flow.add_node(\"international_node\", international_comments)\n",
    "cn_flow.add_node(\"crime_node\", crime_comments)\n",
    "cn_flow.add_node(\"nearby_node\", nearby_comments)\n",
    "cn_flow.add_node(\"finance_node\", finance_comments)\n",
    "cn_flow.add_node(\"salary_node\", salary_comments)\n",
    "cn_flow.add_node(\"intro_node\", intro_info)\n",
    "cn_flow.add_node(\"update_node\", update_cn)\n",
    "\n",
    "cn_flow.add_edge(START, \"initial_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"rank_admin_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"information_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"majors_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"SAT_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"students_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"age_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"international_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"crime_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"nearby_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"finance_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"salary_node\")\n",
    "cn_flow.add_edge(\"initial_node\", \"intro_node\")\n",
    "\n",
    "cn_flow.add_edge(\"rank_admin_node\", \"update_node\")\n",
    "cn_flow.add_edge(\"information_node\", \"update_node\")\n",
    "cn_flow.add_edge(\"majors_node\", \"update_node\")\n",
    "cn_flow.add_edge(\"SAT_node\", \"update_node\")\n",
    "cn_flow.add_edge(\"students_node\", \"update_node\")\n",
    "cn_flow.add_edge(\"age_node\", \"update_node\")\n",
    "cn_flow.add_edge(\"international_node\", \"update_node\")\n",
    "cn_flow.add_edge(\"crime_node\", \"update_node\")\n",
    "cn_flow.add_edge(\"nearby_node\", \"update_node\")\n",
    "cn_flow.add_edge(\"finance_node\", \"update_node\")\n",
    "cn_flow.add_edge(\"salary_node\", \"update_node\")\n",
    "cn_flow.add_edge(\"intro_node\", \"update_node\")\n",
    "\n",
    "cn_flow.add_edge(\"update_node\", END)\n",
    "cn_app = cn_flow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a4f5792e-9a76-4036-9ca3-ab0fda4f40cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class enState(TypedDict):\n",
    "    index: int\n",
    "    comment_string: str\n",
    "    language_string: str\n",
    "    college_postid: str\n",
    "    college_fpusid: str\n",
    "    college_unitid: str\n",
    "    college_name: str\n",
    "    college_cname: str\n",
    "    fpus: str\n",
    "    rank_admin: str\n",
    "    information: str\n",
    "    majors: str\n",
    "    SAT_ACT: str\n",
    "    stduents_comp: str\n",
    "    age: str\n",
    "    international: str\n",
    "    crime: str\n",
    "    nearby: str\n",
    "    intro: str\n",
    "\n",
    "\n",
    "en_flow = StateGraph(enState)\n",
    "en_flow.add_node(\"initial_node\", initial_college_info_en)\n",
    "en_flow.add_node(\"rank_admin_node\", rank_admin_comments)\n",
    "en_flow.add_node(\"information_node\", information_comments)\n",
    "en_flow.add_node(\"majors_node\", majors_comments)\n",
    "en_flow.add_node(\"SAT_node\", SAT_comments)\n",
    "en_flow.add_node(\"students_node\", students_comments)\n",
    "en_flow.add_node(\"age_node\", age_comments)\n",
    "en_flow.add_node(\"international_node\", international_comments)\n",
    "en_flow.add_node(\"crime_node\", crime_comments)\n",
    "en_flow.add_node(\"nearby_node\", nearby_comments)\n",
    "en_flow.add_node(\"intro_node\", intro_info)\n",
    "en_flow.add_node(\"update_node\", update_en)\n",
    "\n",
    "en_flow.add_edge(START, \"initial_node\")\n",
    "en_flow.add_edge(\"initial_node\", \"rank_admin_node\")\n",
    "en_flow.add_edge(\"initial_node\", \"information_node\")\n",
    "en_flow.add_edge(\"initial_node\", \"majors_node\")\n",
    "en_flow.add_edge(\"initial_node\", \"SAT_node\")\n",
    "en_flow.add_edge(\"initial_node\", \"students_node\")\n",
    "en_flow.add_edge(\"initial_node\", \"age_node\")\n",
    "en_flow.add_edge(\"initial_node\", \"international_node\")\n",
    "en_flow.add_edge(\"initial_node\", \"crime_node\")\n",
    "en_flow.add_edge(\"initial_node\", \"nearby_node\")\n",
    "en_flow.add_edge(\"initial_node\", \"intro_node\")\n",
    "\n",
    "en_flow.add_edge(\"rank_admin_node\", \"update_node\")\n",
    "en_flow.add_edge(\"information_node\", \"update_node\")\n",
    "en_flow.add_edge(\"majors_node\", \"update_node\")\n",
    "en_flow.add_edge(\"SAT_node\", \"update_node\")\n",
    "en_flow.add_edge(\"students_node\", \"update_node\")\n",
    "en_flow.add_edge(\"age_node\", \"update_node\")\n",
    "en_flow.add_edge(\"international_node\", \"update_node\")\n",
    "en_flow.add_edge(\"crime_node\", \"update_node\")\n",
    "en_flow.add_edge(\"nearby_node\", \"update_node\")\n",
    "en_flow.add_edge(\"intro_node\", \"update_node\")\n",
    "\n",
    "en_flow.add_edge(\"update_node\", END)\n",
    "en_app = en_flow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3d500-fa03-4a3d-b6ef-0c4f9e2b9fd0",
   "metadata": {},
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph(xray=1).draw_mermaid_png(\n",
    "            curve_style=CurveStyle.BASIS,\n",
    "            node_colors=NodeStyles(\n",
    "                first=\"fill:#FDFFB6\",\n",
    "                last=\"fill:#FFADAD\",\n",
    "                default=\"fill:#CAFFBF,line-height:3\",\n",
    "            ),\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        ),\n",
    "        width=2500,\n",
    "    )\n",
    ")\n",
    "img = app.get_graph(xray=1).draw_mermaid_png(\n",
    "    curve_style=CurveStyle.BASIS,\n",
    "    node_colors=NodeStyles(\n",
    "        first=\"fill:#FDFFB6\",\n",
    "        last=\"fill:#FFADAD\",\n",
    "        default=\"fill:#CAFFBF,line-height:1\",\n",
    "    ),\n",
    "    draw_method=MermaidDrawMethod.API,\n",
    ")\n",
    "with open(\"LLM_data_comments_flow.png\", \"wb\") as png:\n",
    "    png.write(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4693422c-ae9d-4e8d-b6ff-bb901b7c1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "colleges = get_colleges_list(post_type=\"postid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ec230339-ba93-44bb-8be8-82c21220deb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ------done!\n",
      "1 ------done!\n",
      "2 ------done!\n",
      "3 ------done!\n",
      "4 ------done!\n",
      "5 ------done!\n",
      "6 ------done!\n",
      "7 ------done!\n",
      "8 ------done!\n",
      "9 ------done!\n",
      "10 ------done!\n",
      "11 ------done!\n",
      "12 ------done!\n",
      "13 ------done!\n",
      "14 ------done!\n",
      "15 ------done!\n",
      "16 ------done!\n",
      "17 ------done!\n",
      "18 ------done!\n",
      "19 ------done!\n",
      "20 ------done!\n",
      "21 ------done!\n",
      "22 ------done!\n",
      "23 ------done!\n",
      "24 ------done!\n",
      "25 ------done!\n",
      "26 ------done!\n",
      "27 ------done!\n",
      "28 ------done!\n",
      "29 ------done!\n",
      "30 ------done!\n",
      "31 ------done!\n",
      "32 ------done!\n",
      "33 ------done!\n",
      "34 ------done!\n",
      "35 ------done!\n",
      "36 ------done!\n",
      "37 ------done!\n",
      "38 ------done!\n",
      "39 ------done!\n",
      "40 ------done!\n",
      "41 ------done!\n",
      "42 ------done!\n",
      "43 ------done!\n",
      "44 ------done!\n",
      "45 ------done!\n",
      "46 ------done!\n",
      "47 ------done!\n",
      "48 ------done!\n",
      "49 ------done!\n",
      "50 ------done!\n",
      "51 ------done!\n",
      "52 ------done!\n",
      "53 ------done!\n",
      "54 ------done!\n",
      "55 ------done!\n",
      "56 ------done!\n",
      "57 ------done!\n",
      "58 ------done!\n",
      "59 ------done!\n",
      "60 ------done!\n",
      "61 ------done!\n",
      "62 ------done!\n",
      "63 ------done!\n",
      "64 ------done!\n",
      "65 ------done!\n",
      "66 ------done!\n",
      "67 ------done!\n",
      "68 ------done!\n",
      "69 ------done!\n",
      "70 ------done!\n",
      "71 ------done!\n",
      "72 ------done!\n",
      "73 ------done!\n",
      "74 ------done!\n",
      "75 ------done!\n",
      "76 ------done!\n",
      "77 ------done!\n",
      "78 ------done!\n",
      "79 ------done!\n",
      "80 ------done!\n",
      "81 ------done!\n",
      "82 ------done!\n",
      "83 ------done!\n",
      "84 ------done!\n",
      "85 ------done!\n",
      "86 ------done!\n",
      "87 ------done!\n",
      "88 ------done!\n",
      "89 ------done!\n",
      "90 ------done!\n",
      "91 ------done!\n",
      "92 ------done!\n",
      "93 ------done!\n",
      "94 ------done!\n",
      "95 ------done!\n",
      "96 ------done!\n",
      "97 ------done!\n",
      "98 ------done!\n",
      "99 ------done!\n",
      "100 ------done!\n",
      "101 ------done!\n",
      "102 ------done!\n",
      "103 ------done!\n",
      "104 ------done!\n",
      "105 ------done!\n",
      "106 ------done!\n",
      "107 ------done!\n",
      "108 ------done!\n",
      "109 ------done!\n",
      "110 ------done!\n",
      "111 ------done!\n",
      "112 ------done!\n",
      "113 ------done!\n",
      "114 ------done!\n",
      "115 ------done!\n",
      "116 ------done!\n",
      "117 ------done!\n",
      "118 ------done!\n",
      "119 ------done!\n",
      "120 ------done!\n",
      "121 ------done!\n",
      "122 ------done!\n",
      "123 ------done!\n",
      "124 ------done!\n",
      "125 ------done!\n",
      "126 ------done!\n",
      "127 ------done!\n",
      "128 ------done!\n",
      "129 ------done!\n",
      "130 ------done!\n",
      "131 ------done!\n",
      "132 ------done!\n",
      "133 ------done!\n",
      "134 ------done!\n",
      "135 ------done!\n",
      "136 ------done!\n",
      "137 ------done!\n",
      "138 ------done!\n",
      "139 ------done!\n",
      "140 ------done!\n",
      "141 ------done!\n",
      "142 ------done!\n",
      "143 ------done!\n",
      "144 ------done!\n",
      "145 ------done!\n",
      "146 ------done!\n",
      "147 ------done!\n",
      "148 ------done!\n",
      "149 ------done!\n",
      "150 ------done!\n",
      "151 ------done!\n",
      "152 ------done!\n",
      "153 ------done!\n",
      "154 ------done!\n",
      "155 ------done!\n",
      "156 ------done!\n",
      "157 ------done!\n",
      "158 ------done!\n",
      "159 ------done!\n",
      "160 ------done!\n",
      "161 ------done!\n",
      "162 ------done!\n",
      "163 ------done!\n",
      "164 ------done!\n",
      "165 ------done!\n",
      "166 ------done!\n",
      "167 ------done!\n",
      "168 ------done!\n",
      "169 ------done!\n",
      "170 ------done!\n",
      "171 ------done!\n",
      "172 ------done!\n",
      "173 ------done!\n",
      "174 ------done!\n",
      "175 ------done!\n",
      "176 ------done!\n",
      "177 ------done!\n",
      "178 ------done!\n",
      "179 ------done!\n",
      "180 ------done!\n",
      "181 ------done!\n",
      "182 ------done!\n",
      "183 ------done!\n",
      "184 ------done!\n",
      "185 ------done!\n",
      "186 ------done!\n",
      "187 ------done!\n",
      "188 ------done!\n",
      "189 ------done!\n",
      "190 ------done!\n",
      "191 ------done!\n",
      "192 ------done!\n",
      "193 ------done!\n",
      "194 ------done!\n",
      "195 ------done!\n",
      "196 ------done!\n",
      "197 ------done!\n",
      "198 ------done!\n",
      "199 ------done!\n",
      "200 ------done!\n",
      "201 ------done!\n",
      "202 ------done!\n",
      "203 ------done!\n",
      "204 ------done!\n",
      "205 ------done!\n",
      "206 ------done!\n",
      "207 ------done!\n",
      "208 ------done!\n",
      "209 ------done!\n",
      "210 ------done!\n",
      "211 ------done!\n",
      "212 ------done!\n",
      "213 ------done!\n",
      "214 ------done!\n",
      "215 ------done!\n",
      "216 ------done!\n",
      "217 ------done!\n",
      "218 ------done!\n",
      "219 ------done!\n",
      "220 ------done!\n",
      "221 ------done!\n",
      "222 ------done!\n",
      "223 ------done!\n",
      "224 ------done!\n",
      "225 ------done!\n",
      "226 ------done!\n",
      "227 ------done!\n",
      "228 ------done!\n",
      "229 ------done!\n",
      "230 ------done!\n",
      "231 ------done!\n",
      "232 ------done!\n",
      "233 ------done!\n",
      "234 ------done!\n",
      "235 ------done!\n",
      "236 ------done!\n",
      "237 ------done!\n",
      "238 ------done!\n",
      "239 ------done!\n",
      "240 ------done!\n",
      "241 ------done!\n",
      "242 ------done!\n",
      "243 ------done!\n",
      "244 ------done!\n",
      "245 ------done!\n",
      "246 ------done!\n",
      "247 ------done!\n",
      "248 ------done!\n",
      "249 ------done!\n",
      "250 ------done!\n",
      "251 ------done!\n",
      "252 ------done!\n",
      "253 ------done!\n",
      "254 ------done!\n",
      "255 ------done!\n",
      "256 ------done!\n",
      "257 ------done!\n",
      "258 ------done!\n",
      "259 ------done!\n",
      "260 ------done!\n",
      "261 ------done!\n",
      "262 ------done!\n",
      "263 ------done!\n",
      "264 ------done!\n",
      "265 ------done!\n",
      "266 ------done!\n",
      "267 ------done!\n",
      "268 ------done!\n",
      "269 ------done!\n",
      "270 ------done!\n",
      "271 ------done!\n",
      "272 ------done!\n",
      "273 ------done!\n",
      "274 ------done!\n",
      "275 ------done!\n",
      "276 ------done!\n",
      "277 ------done!\n",
      "278 ------done!\n",
      "279 ------done!\n",
      "280 ------done!\n",
      "281 ------done!\n",
      "282 ------done!\n",
      "283 ------done!\n",
      "284 ------done!\n",
      "285 ------done!\n",
      "286 ------done!\n",
      "287 ------done!\n",
      "288 ------done!\n",
      "289 ------done!\n",
      "290 ------done!\n",
      "291 ------done!\n",
      "292 ------done!\n",
      "293 ------done!\n",
      "294 ------done!\n",
      "295 ------done!\n",
      "296 ------done!\n",
      "297 ------done!\n",
      "298 ------done!\n",
      "299 ------done!\n",
      "300 ------done!\n",
      "301 ------done!\n",
      "302 ------done!\n",
      "303 ------done!\n",
      "304 ------done!\n",
      "305 ------done!\n",
      "306 ------done!\n",
      "307 ------done!\n",
      "308 ------done!\n",
      "309 ------done!\n",
      "310 ------done!\n",
      "311 ------done!\n",
      "312 ------done!\n",
      "313 ------done!\n",
      "314 ------done!\n",
      "315 ------done!\n",
      "316 ------done!\n",
      "317 ------done!\n",
      "318 ------done!\n",
      "319 ------done!\n",
      "320 ------done!\n",
      "321 ------done!\n",
      "322 ------done!\n",
      "323 ------done!\n",
      "324 ------done!\n",
      "325 ------done!\n",
      "326 ------done!\n",
      "327 ------done!\n",
      "328 ------done!\n",
      "329 ------done!\n",
      "330 ------done!\n",
      "331 ------done!\n",
      "332 ------done!\n",
      "333 ------done!\n",
      "334 ------done!\n",
      "335 ------done!\n",
      "336 ------done!\n",
      "337 ------done!\n",
      "338 ------done!\n",
      "339 ------done!\n",
      "340 ------done!\n",
      "341 ------done!\n",
      "342 ------done!\n",
      "343 ------done!\n",
      "344 ------done!\n",
      "345 ------done!\n",
      "346 ------done!\n",
      "347 ------done!\n",
      "348 ------done!\n",
      "349 ------done!\n",
      "350 ------done!\n",
      "351 ------done!\n",
      "352 ------done!\n",
      "353 ------done!\n",
      "354 ------done!\n",
      "355 ------done!\n",
      "356 ------done!\n",
      "357 ------done!\n",
      "358 ------done!\n",
      "359 ------done!\n",
      "360 ------done!\n",
      "361 ------done!\n",
      "362 ------done!\n",
      "363 ------done!\n",
      "364 ------done!\n",
      "365 ------done!\n",
      "366 ------done!\n",
      "367 ------done!\n",
      "368 ------done!\n",
      "369 ------done!\n",
      "370 ------done!\n",
      "371 ------done!\n",
      "372 ------done!\n",
      "373 ------done!\n",
      "374 ------done!\n",
      "375 ------done!\n",
      "376 ------done!\n",
      "377 ------done!\n",
      "378 ------done!\n",
      "379 ------done!\n",
      "380 ------done!\n",
      "381 ------done!\n",
      "382 ------done!\n",
      "383 ------done!\n",
      "384 ------done!\n",
      "385 ------done!\n",
      "386 ------done!\n",
      "387 ------done!\n",
      "388 ------done!\n",
      "389 ------done!\n",
      "390 ------done!\n",
      "391 ------done!\n",
      "392 ------done!\n",
      "393 ------done!\n",
      "394 ------done!\n",
      "395 ------done!\n",
      "396 ------done!\n",
      "397 ------done!\n",
      "398 ------done!\n",
      "399 ------done!\n",
      "400 ------done!\n",
      "401 ------done!\n",
      "402 ------done!\n",
      "403 ------done!\n",
      "404 ------done!\n",
      "405 ------done!\n",
      "406 ------done!\n",
      "407 ------done!\n",
      "408 ------done!\n",
      "409 ------done!\n",
      "410 ------done!\n",
      "411 ------done!\n",
      "412 ------done!\n",
      "413 ------done!\n",
      "414 ------done!\n",
      "415 ------done!\n",
      "416 ------done!\n",
      "417 ------done!\n",
      "418 ------done!\n",
      "419 ------done!\n",
      "420 ------done!\n",
      "421 ------done!\n",
      "422 ------done!\n",
      "423 ------done!\n",
      "424 ------done!\n",
      "425 ------done!\n",
      "426 ------done!\n",
      "427 ------done!\n",
      "428 ------done!\n",
      "429 ------done!\n",
      "430 ------done!\n",
      "431 ------done!\n",
      "432 ------done!\n",
      "433 ------done!\n",
      "434 ------done!\n",
      "435 ------done!\n",
      "436 ------done!\n",
      "437 ------done!\n",
      "438 ------done!\n",
      "439 ------done!\n",
      "440 ------done!\n",
      "441 ------done!\n",
      "442 ------done!\n",
      "443 ------done!\n",
      "444 ------done!\n",
      "445 ------done!\n",
      "446 ------done!\n",
      "447 ------done!\n",
      "448 ------done!\n",
      "449 ------done!\n",
      "450 ------done!\n",
      "451 ------done!\n",
      "452 ------done!\n",
      "453 ------done!\n",
      "454 ------done!\n",
      "455 ------done!\n",
      "456 ------done!\n",
      "457 ------done!\n",
      "458 ------done!\n",
      "459 ------done!\n",
      "460 ------done!\n",
      "461 ------done!\n",
      "462 ------done!\n",
      "463 ------done!\n",
      "464 ------done!\n",
      "465 ------done!\n",
      "466 ------done!\n",
      "467 ------done!\n",
      "468 ------done!\n",
      "469 ------done!\n",
      "470 ------done!\n",
      "471 ------done!\n",
      "472 ------done!\n",
      "473 ------done!\n",
      "474 ------done!\n",
      "475 ------done!\n",
      "476 ------done!\n",
      "477 ------done!\n",
      "478 ------done!\n",
      "479 ------done!\n",
      "480 ------done!\n",
      "481 ------done!\n",
      "482 ------done!\n",
      "483 ------done!\n",
      "484 ------done!\n",
      "485 ------done!\n",
      "486 ------done!\n",
      "487 ------done!\n",
      "488 ------done!\n",
      "489 ------done!\n",
      "490 ------done!\n",
      "491 ------done!\n",
      "492 ------done!\n",
      "493 ------done!\n",
      "494 ------done!\n",
      "495 ------done!\n",
      "496 ------done!\n",
      "497 ------done!\n",
      "498 ------done!\n",
      "499 ------done!\n",
      "500 ------done!\n",
      "501 ------done!\n",
      "502 ------done!\n",
      "503 ------done!\n",
      "504 ------done!\n",
      "505 ------done!\n",
      "506 ------done!\n",
      "507 ------done!\n",
      "508 ------done!\n",
      "509 ------done!\n",
      "510 ------done!\n",
      "511 ------done!\n",
      "512 ------done!\n",
      "513 ------done!\n",
      "514 ------done!\n",
      "515 ------done!\n",
      "516 ------done!\n",
      "517 ------done!\n",
      "518 ------done!\n",
      "519 ------done!\n",
      "520 ------done!\n",
      "521 ------done!\n",
      "522 ------done!\n",
      "523 ------done!\n",
      "524 ------done!\n",
      "525 ------done!\n",
      "526 ------done!\n",
      "527 ------done!\n",
      "528 ------done!\n",
      "529 ------done!\n",
      "530 ------done!\n",
      "531 ------done!\n",
      "532 ------done!\n",
      "533 ------done!\n",
      "534 ------done!\n",
      "535 ------done!\n",
      "536 ------done!\n",
      "537 ------done!\n",
      "538 ------done!\n",
      "539 ------done!\n",
      "540 ------done!\n",
      "541 ------done!\n",
      "542 ------done!\n",
      "543 ------done!\n",
      "544 ------done!\n",
      "545 ------done!\n",
      "546 ------done!\n",
      "547 ------done!\n",
      "548 ------done!\n",
      "549 ------done!\n",
      "550 ------done!\n",
      "551 ------done!\n",
      "552 ------done!\n",
      "553 ------done!\n",
      "554 ------done!\n",
      "555 ------done!\n",
      "556 ------done!\n",
      "557 ------done!\n",
      "558 ------done!\n",
      "559 ------done!\n",
      "560 ------done!\n",
      "561 ------done!\n",
      "562 ------done!\n"
     ]
    }
   ],
   "source": [
    "for i in range(colleges.shape[0]):\n",
    "    cn_app.invoke({\"index\": i})\n",
    "    print(i, \"------done!\")\n",
    "    #time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d5a3463e-f8ac-4f7d-a051-faa2420a1b20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(colleges.shape[0]):\n",
    "#     en_app.invoke({\"index\": i})\n",
    "#     print(i, \"------done!\")\n",
    "#     time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
