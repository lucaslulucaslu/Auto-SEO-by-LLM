{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3cab3aa-d600-4e0c-8b94-5b4064d75a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "from typing import List, Literal, TypedDict\n",
    "\n",
    "import markdown\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import requests\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from LLM_get_folder import get_local_folder\n",
    "from PIL import Image\n",
    "#from langchain.docstore.document import Document\n",
    "model_small = \"gpt-3.5-turbo\"\n",
    "\n",
    "model_large = \"gpt-4o\"\n",
    "\n",
    "llm_small = ChatOpenAI(model=model_small, temperature=0, timeout=40000)\n",
    "llm_large = ChatOpenAI(model=model_large, temperature=0, timeout=40000)\n",
    "# llm = ChatOpenAI()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"college-news-auto-post\"\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "MAX_WEB_URL = 5\n",
    "MAX_QUERY_RESULT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b31f868-5914-450c-8dc2-a4a003ad286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    url: str\n",
    "    summary: str\n",
    "    topics: List[str]\n",
    "    documents: str\n",
    "    image_query: str\n",
    "    title: str\n",
    "    title_en:str\n",
    "    content: str\n",
    "    content_en:str\n",
    "    tags: set\n",
    "    tags_en:set\n",
    "    image_url: str\n",
    "    image_url_en:str\n",
    "    image_ID:int\n",
    "    image_ID_en: int\n",
    "    image_filename: str\n",
    "    reference: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e0c942-2c1f-4f2c-a63b-71682195898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class summary_output(BaseModel):\n",
    "    title: str = Field(description=\"网页内容总结标题，标题的语言必须是英文\")\n",
    "    summary: str = Field(description=\"网页内容总结内容，总结内容的语言必须是中文\")\n",
    "\n",
    "\n",
    "def summary_from_url(url):\n",
    "    \"\"\"\n",
    "    from langchain_community.document_loaders import SeleniumURLLoader\n",
    "    loader =SeleniumURLLoader(urls=[])\n",
    "    docs=loader.load()\n",
    "    \"\"\"\n",
    "    summary_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"你是一名专业的新闻总结员，下面给出的网页内容是一篇新闻，请从该新闻内容总结一段800个汉字左右的总结，总结内容必须使用中文输出。总结要全面，字数大约在800个汉字左右。\\\n",
    "                同时要提取该新闻内容的标题，如果没有标题的可以通过内容总结一个标题。\\\n",
    "                如果网页内容中包含新闻发布日期的，请在总结中也包含该日期，日期要包含年份。如果网页内容中提到美国大学的，请在总结中也包含该大学的相关内容。\"\"\",\n",
    "            ),\n",
    "            (\"human\", \"网页内容: {content}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        loader = WebBaseLoader(\n",
    "            url, requests_kwargs={\"timeout\": 10}, raise_for_status=True\n",
    "        )\n",
    "        # loader.requests_kwargs = {\"verify\": False}\n",
    "        docs = loader.load()\n",
    "        content = docs[0].page_content\n",
    "        encoding = tiktoken.encoding_for_model(model_small)\n",
    "        token_length = len(encoding.encode(content))\n",
    "        if token_length <= 15000:\n",
    "            structured_llm = llm_small.with_structured_output(summary_output)\n",
    "        else:\n",
    "            structured_llm = llm_large.with_structured_output(summary_output)\n",
    "        summary_chain = summary_prompt | structured_llm\n",
    "        response = summary_chain.invoke({\"content\": content})\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 404:\n",
    "            response = \"404\"\n",
    "        response = None\n",
    "    except:\n",
    "        response = None\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c397edf-8768-42f7-a657-f94834ad31eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(state):\n",
    "    url = state[\"url\"]\n",
    "    response = summary_from_url(url)\n",
    "    if response == \"404\":\n",
    "        set_url_flag(url)\n",
    "        raise Exception(\"Orginal url 404 error\")\n",
    "    title = response.title\n",
    "    summary = response.summary\n",
    "    print(\"Finish Initial Summary: \", url)\n",
    "    url_base = url.split(\"?\")[0]\n",
    "    return {\"summary\": summary, \"reference\": {url_base: title}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5785bf-d1e7-4db8-9796-46db2331b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class more_points(BaseModel):\n",
    "    more: list[str] = Field(\n",
    "        description=\"可以展开讨论的点的搜索查询，必须是英文搜索查询\"\n",
    "    )\n",
    "\n",
    "\n",
    "def more_topics(state):\n",
    "    summary = state[\"summary\"]\n",
    "    structured_llm = llm_large.with_structured_output(more_points)\n",
    "    more_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"从下面用户给出的一段原始总结文字中提出不少于'两'点但不多于'四'点与原文有联系且可以展开讨论的点, 请给出详细的搜索查询来代表这几个点，搜索查询必须要完整且详细，\n",
    "                搜索查询必须为英文，不管输入内容是什么语言，输出的搜索查询必须是英文查询。\"\"\",\n",
    "            ),\n",
    "            (\"human\", \"原始总结文字：{summary}\"),\n",
    "        ]\n",
    "    )\n",
    "    more_chain = more_prompt | structured_llm\n",
    "    topics = more_chain.invoke({\"summary\": summary})\n",
    "    return {\"topics\": topics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "933f7ba9-6ccb-471d-a6e7-b986154a7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state):\n",
    "    search = GoogleSerperAPIWrapper(type=\"news\")\n",
    "    queries = state[\"topics\"].more\n",
    "    reference = state[\"reference\"]\n",
    "    documents = \"\"\n",
    "    n_urls = 0\n",
    "    for query in queries:\n",
    "        if n_urls >= MAX_WEB_URL:\n",
    "            break\n",
    "        results = search.results(query)\n",
    "        n_results = 0\n",
    "        for result in results[\"news\"]:\n",
    "            url = result[\"link\"]\n",
    "            url_base = url.split(\"?\")[0]\n",
    "            if url_base in reference:\n",
    "                continue\n",
    "            else:\n",
    "                response = summary_from_url(url)\n",
    "                if response is not None:\n",
    "                    print(\"------ Topics Summary: \", url)\n",
    "                    doc = response.summary\n",
    "                    title = response.title\n",
    "                    reference[url_base] = title\n",
    "                    documents = documents + query + \"\\n\\n\" + doc + \"\\n\\n\"\n",
    "                    n_urls = n_urls + 1\n",
    "                    n_results = n_results + 1\n",
    "                    if n_results >= MAX_QUERY_RESULT:\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "    return {\"documents\": documents, \"reference\": reference}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6cbcf86-74c8-4a8c-ad91-1f172cc2f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    summary = state[\"summary\"]\n",
    "    documents = state[\"documents\"]\n",
    "    topics = state[\"topics\"].more\n",
    "    content_array = []\n",
    "    content_set = {}\n",
    "    repeat_N = 0\n",
    "    system_message = \"\"\"你的角色是一名专业的美国大学新闻评论员，下面给出一段原始总结内容，同时给你一些可以扩展讨论的点以及支持这些讨论点的讨论，请重新写一遍2000-4000中文字数的评论文章，字数可以根据内容的多少来适当选择，但不能少于2000字也不能多余4000字。\n",
    "                    评论文章内容应围绕原始总结的内容，对与美国大学相关的内容要额外关注，适当的结合讨论点扩展讨论，在扩展讨论的时候需要前后呼应提及与原始总结相关联的联系，最后需要综合原始总结和扩展讨论再做一个最后的总结，文章前后逻辑关系需要连贯，如果有需要可以重写全文，\n",
    "                    文章中如果遇到美国大学名称，第一次必须使用美国大学的中文全名，之后可以适当使用缩写，美国大学的全名要使用最普遍的中文翻译版本，遇到加利福尼亚大学各个分校一律使用加州大学+分校名称。\n",
    "                    遇到布兰戴斯大学请使用布兰迪斯大学，遇到威廉与玛丽学院请使用威廉玛丽学院，遇到加州大学欧文分校请使用加州大学尔湾分校，遇到利哈伊大学请使用里海大学，遇到密歇根大学安娜堡分校要使用全称，不能缩写成密歇根大学，\n",
    "                    遇到伊利诺伊大学香槟分校要使用全称呼伊利诺伊大学香槟分校，不能缩写成伊利诺伊大学；遇到乔治亚大学请使用佐治亚大学；遇到华盛顿大学圣路易斯分校请用圣路易斯华盛顿大学\n",
    "                    在文章中合适的位置，比如中上部位且在两个段落之间，需要放置一个且必须有一个图片空位，用'[image_placeholder]'代表这个图片空位。\"\"\"\n",
    "    while len(content_array) == 0 or (len(content_array) - len(content_set)) > 0:\n",
    "        rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_message),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"原始总结：\\n\\n{summary},\\n\\n扩展讨论点：\\n\\n{topics},\\n\\n支持扩展讨论点的讨论：\\n\\n{documents}\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        rewrite_chain = rewrite_prompt | llm_large | StrOutputParser()\n",
    "        content = rewrite_chain.invoke(\n",
    "            {\"summary\": summary, \"topics\": topics, \"documents\": documents}\n",
    "        )\n",
    "        content_array = content.split(\"\\n\\n\")\n",
    "        for element in content_array:\n",
    "            if len(element) <= 20:\n",
    "                content_array.remove(element)\n",
    "        content_set = set(content_array)\n",
    "        repeat_N = repeat_N + 1\n",
    "        if repeat_N >= 4:\n",
    "            local_folder = get_local_folder()\n",
    "            file = os.path.join(get_local_folder(), \"repeat.txt\")\n",
    "            f = open(file, \"a\")\n",
    "            f.write(\n",
    "                content\n",
    "                + \"\\n\\n---------------------------------------------------------------------\\n\\n\"\n",
    "            )\n",
    "            f.close()\n",
    "            break\n",
    "        system_message = (\n",
    "            system_message\n",
    "            + \"\\n\\n新的文章不要有重复的段落，如果遇到原始总结内容和扩展讨论点有重复内容的可以忽略重复内容，在最后的输出文章中不可以有重复段落。\"\n",
    "        )\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d0eb04-bdb6-4d2f-8529-85798f30a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\shich\\AppData\\Local\\Temp\\ipykernel_15136\\7333830.py:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  system_message = \"\"\"Your role is a professional American university news commentator. Below is an original summary along with some discussion points and supporting arguments to expand upon.\\\n"
     ]
    }
   ],
   "source": [
    "def rewrite_en(state):\n",
    "    summary = state[\"summary\"]\n",
    "    documents = state[\"documents\"]\n",
    "    topics = state[\"topics\"].more\n",
    "    content_array = []\n",
    "    content_set = {}\n",
    "    repeat_N = 0\n",
    "    system_message = \"\"\"Your role is a professional American university news commentator. Below is an original summary along with some discussion points and supporting arguments to expand upon.\\\n",
    "    Please rewrite a 2000-4000 word commentary article, with the word count adjusted according to the content, but not less than 2000 words and not more than 4000 words.\\\n",
    "    The commentary should center around the original summary, with extra focus on content related to American universities. Appropriately integrate the discussion points and their \\\n",
    "    supporting arguments into your expanded discussion. Ensure to consistently refer back to the original summary when expanding your discussion. \\\n",
    "    Conclude with a comprehensive summary that ties together the original summary and the expanded discussion. The logical flow of the article must be coherent, \\\n",
    "    and the entire text may be rewritten if necessary.\\When mentioning the names of American universities for the first time, always use the full English name. \\\n",
    "    Subsequent mentions can use abbreviations where appropriate. Use the most commonly recognized English version of the full names. \\\n",
    "    For campuses, use a hyphen to connect, such as University of California-Berkeley.\\\n",
    "    In a suitable position within the article, preferably towards the upper-middle and between two paragraphs, include one image placeholder represented by '[image_placeholder]'.\\\n",
    "    The final output must be in English, regardless of any other considerations.\"\"\"\n",
    "    while len(content_array) == 0 or (len(content_array) - len(content_set)) > 0:\n",
    "        rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_message),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"Original summary：\\n\\n{summary},\\n\\nDiscussion Points：\\n\\n{topics},\\n\\nsupporting arguments：\\n\\n{documents}\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        rewrite_chain = rewrite_prompt | llm_large | StrOutputParser()\n",
    "        content = rewrite_chain.invoke(\n",
    "            {\"summary\": summary, \"topics\": topics, \"documents\": documents}\n",
    "        )\n",
    "        content_array = content.split(\"\\n\\n\")\n",
    "        for element in content_array:\n",
    "            if len(element) <= 20:\n",
    "                content_array.remove(element)\n",
    "        content_set = set(content_array)\n",
    "        repeat_N = repeat_N + 1\n",
    "        if repeat_N >= 4:\n",
    "            local_folder = get_local_folder()\n",
    "            file = os.path.join(get_local_folder(), \"repeat.txt\")\n",
    "            f = open(file, \"a\")\n",
    "            f.write(\n",
    "                content\n",
    "                + \"\\n\\n---------------------------------------------------------------------\\n\\n\"\n",
    "            )\n",
    "            f.close()\n",
    "            break\n",
    "        system_message = (\n",
    "            system_message\n",
    "            + \"\\n\\n新的文章不要有重复的段落，如果遇到原始总结内容和扩展讨论点有重复内容的可以忽略重复内容，在最后的输出文章中不可以有重复段落。\"\n",
    "        )\n",
    "    return {\"content_en\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89016558-85c0-450f-8d23-7613f7097793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_format(BaseModel):\n",
    "    title: str = Field(description=\"与文章内容相关的文章中文标题，用中文输出\")\n",
    "    title_en: str = Field(description=\"与文章内容相关的文章英文标题，用英文输出\")\n",
    "    tags: List[str] = Field(description=\"与文章内容相关的中文标签，用中文输出\")\n",
    "    tags_en: List[str] = Field(description=\"与文章内容相关的英文标签，用英文输出\")\n",
    "    image_query: str = Field(\n",
    "        description=\"detailed prompt to generate an image that based on the article content, should be in English\"\n",
    "    )\n",
    "    image_filename: str = Field(\n",
    "        description=\"a good name for the image file without file extension, should be in English\"\n",
    "    )\n",
    "\n",
    "def article_metas(state):\n",
    "    content = state[\"content\"]\n",
    "    meta_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"下面给出一篇文章的内容，请给文章取一个合适的标题，文章标题需要中文和英文两个版本。\n",
    "                同时生成一些与文章内容相关的标签，标签同样需要中文和英文两个版本。\n",
    "                also generate a detailed prompt to generate an image that based on the article content, the image generation prompt should be in English, \\\n",
    "                be careful with prompt words, use safe words that won't violate 'safety system'. The image style should be based on the article content. \\\n",
    "                also generate a image file name without file type extension regarding this image prompt, the image filename should also be in English.\"\"\",\n",
    "            ),\n",
    "            (\"human\", \"content：\\n\\n{content}\"),\n",
    "        ]\n",
    "    )\n",
    "    structured_llm = llm_large.with_structured_output(meta_format)\n",
    "    meta_chain = meta_prompt | structured_llm\n",
    "    response = meta_chain.invoke({\"content\": content})\n",
    "    tag_names = response.tags\n",
    "    tags = tags_to_IDs(tag_names)\n",
    "    tag_names_en=response.tags_en\n",
    "    tags_en=tags_to_IDs_en(tag_names_en)\n",
    "    return {\n",
    "        \"title\": response.title,\n",
    "        \"title_en\": response.title_en,\n",
    "        \"image_query\": response.image_query,\n",
    "        \"image_filename\": response.image_filename,\n",
    "        \"tags\": tags,\n",
    "        \"tags_en\": tags_en,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d16e826-7ada-42dd-9ac0-196dab950e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_list(type='cn'):\n",
    "    connection = pymysql.connect(\n",
    "        db=os.environ[\"db_name\"],\n",
    "        user=os.environ[\"db_user\"],\n",
    "        passwd=os.environ[\"db_pass\"],\n",
    "        host=os.environ[\"db_host\"],\n",
    "        port=3306,\n",
    "        cursorclass=pymysql.cursors.DictCursor,\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    query = \"\"\"select ranking FROM fp_IPEDS.latest_information\"\"\"\n",
    "    cursor.execute(query)\n",
    "    row = cursor.fetchone()\n",
    "    ranking_year = row[\"ranking\"]\n",
    "\n",
    "    if type=='cn':\n",
    "        query = \"\"\"SELECT t1.cname as keyword,concat(\"https://www.forwardpathway.com/\",t1.postid) as url,t2.term_id as tag_id, t3.rank FROM fp_ranking.`colleges` t1\n",
    "    LEFT JOIN fp_forwardpathway.`wp_mmcp_terms` t2 ON t1.cname=REPLACE(t2.name,\"相关新闻\",\"\") AND t2.name LIKE \"%相关新闻\"\n",
    "    LEFT JOIN fp_ranking.us_rankings t3 ON t3.postid=t1.postid AND t3.year={} AND t3.type=1\"\"\".format(\n",
    "            ranking_year\n",
    "        )\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "        keywords_array = []\n",
    "        for row in rows:\n",
    "            keywords_array = keywords_array + [row]\n",
    "        query = \"\"\"SELECT keyword, url,Null,Null FROM fp_chatGPT.keywords\"\"\"\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "        for row in rows:\n",
    "            keywords_array = keywords_array + [row]\n",
    "    else:\n",
    "        query = \"\"\"SELECT t2.post_title as keyword,concat(\"https://www.forwardpathway.us/\",t2.post_name) as url,NULL as tag_id,t4.rank FROM fpus_colleges.transform t1\n",
    "JOIN fpus_wordpress.wp_posts t2 ON t2.ID=t1.postid\n",
    "LEFT JOIN fp_ranking.colleges t3 ON t3.unitid=t1.unitid\n",
    "LEFT JOIN fp_ranking.us_rankings t4 ON t4.postid=t3.postid AND t4.type=1 AND t4.year={}\"\"\".format(\n",
    "            ranking_year\n",
    "        )\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "        keywords_array = []\n",
    "        for row in rows:\n",
    "            keywords_array = keywords_array + [row]\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    keywords = pd.DataFrame(\n",
    "        keywords_array, columns=[\"keyword\", \"url\", \"tag_id\", \"rank\"]\n",
    "    )\n",
    "    keywords = keywords.reset_index(drop=True)\n",
    "\n",
    "    keywords = keywords.reindex(\n",
    "        keywords[\"keyword\"].str.len().sort_values(ascending=False).index\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return (ranking_year, keywords)\n",
    "\n",
    "\n",
    "def insert_keyword_url(content,type='cn'):\n",
    "    soup = BeautifulSoup(content)\n",
    "    tags = set()\n",
    "    n = 0\n",
    "    if type=='cn':\n",
    "        ranking_string=\"\"\"（{}USNews<a href=\"https://www.forwardpathway.com/ranking\">美国大学排名</a>：{}）\"\"\"\n",
    "        (ranking_year, keywords) = get_keywords_list(type='cn')\n",
    "    else:\n",
    "        ranking_string=\"\"\" (<a href=\"https://www.forwardpathway.com/ranking\">{} USNews Ranking</a>: {}) \"\"\"\n",
    "        (ranking_year, keywords) = get_keywords_list(type='en')\n",
    "    for key, row in keywords.iterrows():\n",
    "        keyword = row[\"keyword\"]\n",
    "        url = row[\"url\"]\n",
    "        tag_id = row[\"tag_id\"]\n",
    "        rank = row[\"rank\"]\n",
    "        new_tag = soup.new_tag(\"a\", href=url)\n",
    "        new_tag.string = keyword\n",
    "        void_tags_a = [\"a\"]\n",
    "        void_tags_h = [\"h1\", \"h2\", \"h3\", \"h4\"]\n",
    "        pattern = re.compile(keyword)\n",
    "        results = soup.find_all(string=pattern)\n",
    "        for string_element in results:\n",
    "            parents_set = set([x.name for x in string_element.parents])\n",
    "            if any([x in parents_set for x in void_tags_a]):\n",
    "                continue\n",
    "            if not pd.isna(tag_id):\n",
    "                tags.add(tag_id)\n",
    "            if any([x in parents_set for x in void_tags_h]):\n",
    "                continue\n",
    "            if pd.isna(rank) or n > 0:\n",
    "                new_element = BeautifulSoup(\n",
    "                    string_element.replace(keyword, str(new_tag), 1), \"html.parser\"\n",
    "                )\n",
    "            else:\n",
    "                new_element = BeautifulSoup(\n",
    "                    string_element.replace(\n",
    "                        keyword,\n",
    "                        str(new_tag)\n",
    "                        + ranking_string.format(\n",
    "                            ranking_year, int(rank)\n",
    "                        ),\n",
    "                        1,\n",
    "                    ),\n",
    "                    \"html.parser\",\n",
    "                )\n",
    "            string_element = string_element.replace_with(new_element)\n",
    "            n = n + 1\n",
    "            break\n",
    "    return (str(soup), tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3babcedf-31fa-4bfa-9003-69627a21c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_url = \"https://www.forwardpathway.com/wp-json/wp/v2\"\n",
    "wp_post_url = wp_url + \"/posts\"\n",
    "wp_media_url = wp_url + \"/media\"\n",
    "wp_tag_url = wp_url + \"/tags\"\n",
    "\n",
    "wp_url_en = \"https://www.forwardpathway.us/wp-json/wp/v2\"\n",
    "wp_post_url_en = wp_url_en + \"/posts\"\n",
    "wp_media_url_en = wp_url_en + \"/media\"\n",
    "wp_tag_url_en = wp_url_en + \"/tags\"\n",
    "\n",
    "user_id = os.environ[\"wordpress_username\"]\n",
    "# user app password can be created in the user/edit user/application password\n",
    "user_app_password = os.environ[\"wordpress_pass\"]\n",
    "credentials = user_id + \":\" + user_app_password\n",
    "token = base64.b64encode(credentials.encode())\n",
    "header = {\"Authorization\": \"Basic \" + token.decode(\"utf-8\")}\n",
    "\n",
    "user_id_en = os.environ[\"wordpress_username_fpus\"]\n",
    "# user app password can be created in the user/edit user/application password\n",
    "user_app_password_en = os.environ[\"wordpress_pass_fpus\"]\n",
    "credentials_en = user_id_en + \":\" + user_app_password_en\n",
    "token_en = base64.b64encode(credentials_en.encode())\n",
    "header_en = {\"Authorization\": \"Basic \" + token_en.decode(\"utf-8\")}\n",
    "\n",
    "def tags_to_IDs(tag_names=[]):\n",
    "    tags = set()\n",
    "    connection = pymysql.connect(\n",
    "        db=os.environ[\"db_name\"],\n",
    "        user=os.environ[\"db_user\"],\n",
    "        passwd=os.environ[\"db_pass\"],\n",
    "        host=os.environ[\"db_host\"],\n",
    "        port=3306,\n",
    "        cursorclass=pymysql.cursors.DictCursor,\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    for tag_name in tag_names:\n",
    "        query = \"\"\"SELECT t1.term_id FROM fp_forwardpathway.wp_mmcp_terms t1 JOIN fp_forwardpathway.wp_mmcp_term_taxonomy t2 ON t2.term_id=t1.term_id AND t2.taxonomy=\"post_tag\" WHERE t1.name=%s\"\"\"\n",
    "        rows_count = cursor.execute(query, tag_name)\n",
    "        if rows_count > 0:\n",
    "            result = cursor.fetchone()\n",
    "            tags.add(result[\"term_id\"])\n",
    "        else:\n",
    "            tag_data = {\"name\": tag_name}\n",
    "            response = requests.post(wp_tag_url, headers=header, json=tag_data)\n",
    "            tags.add(response.json()[\"id\"])\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    return tags\n",
    "    \n",
    "def tags_to_IDs_en(tag_names=[]):\n",
    "    tags = set()\n",
    "    connection = pymysql.connect(\n",
    "        db=os.environ[\"db_name\"],\n",
    "        user=os.environ[\"db_user\"],\n",
    "        passwd=os.environ[\"db_pass\"],\n",
    "        host=os.environ[\"db_host\"],\n",
    "        port=3306,\n",
    "        cursorclass=pymysql.cursors.DictCursor,\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    for tag_name in tag_names:\n",
    "        query = \"\"\"SELECT t1.term_id FROM fpus_wordpress.wp_terms t1 JOIN fpus_wordpress.wp_term_taxonomy t2 ON t2.term_id=t1.term_id AND t2.taxonomy=\"post_tag\" WHERE t1.name=%s\"\"\"\n",
    "        rows_count = cursor.execute(query, tag_name)\n",
    "        if rows_count > 0:\n",
    "            result = cursor.fetchone()\n",
    "            tags.add(result[\"term_id\"])\n",
    "        else:\n",
    "            tag_data = {\"name\": tag_name}\n",
    "            response = requests.post(wp_tag_url_en, headers=header_en, json=tag_data)\n",
    "            tags.add(response.json()[\"id\"])\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    return tags\n",
    "\n",
    "def post_post(\n",
    "    article_title, article_body, post_status=\"publish\", featured_media_id=0, tags={}\n",
    "):\n",
    "    post_data = {\n",
    "        \"title\": article_title,\n",
    "        \"content\": article_body,\n",
    "        \"comment_status\": \"closed\",\n",
    "        \"categories\": [3627],  # 美国大学相关新闻category\n",
    "        \"tags\": list(tags),\n",
    "        \"status\": post_status,\n",
    "        \"featured_media\": featured_media_id,\n",
    "    }\n",
    "    response = requests.post(wp_post_url, headers=header, json=post_data)\n",
    "    return response\n",
    "\n",
    "def post_post_en(\n",
    "    article_title, article_body, post_status=\"publish\", featured_media_id=0, tags={}\n",
    "):\n",
    "    post_data = {\n",
    "        \"title\": article_title,\n",
    "        \"content\": article_body,\n",
    "        \"comment_status\": \"closed\",\n",
    "        \"categories\": [9],  # 美国大学相关新闻category\n",
    "        \"tags\": list(tags),\n",
    "        \"status\": post_status,\n",
    "        \"featured_media\": featured_media_id,\n",
    "    }\n",
    "    response = requests.post(wp_post_url_en, headers=header_en, json=post_data)\n",
    "    return response\n",
    "    \n",
    "def post_file(file_path):\n",
    "    media = {\n",
    "        \"file\": open(file_path, \"rb\"),\n",
    "        \"caption\": \"LLM_auto_post_file_\" + file_path,\n",
    "    }\n",
    "    response = requests.post(wp_media_url, headers=header, files=media)\n",
    "    return response\n",
    "\n",
    "def post_file_en(file_path):\n",
    "    media = {\n",
    "        \"file\": open(file_path, \"rb\"),\n",
    "        \"caption\": \"LLM_auto_post_file_\" + file_path,\n",
    "    }\n",
    "    response = requests.post(wp_media_url_en, headers=header_en, files=media)\n",
    "    return response\n",
    "\n",
    "def generate_image(state):\n",
    "    image_filename = state[\"image_filename\"]\n",
    "    image_folder = os.path.join(get_local_folder(), \"images\")\n",
    "    png_image = os.path.join(image_folder, image_filename + \".png\")\n",
    "    jpg_image = os.path.join(image_folder, image_filename + \".jpg\")\n",
    "    image_query = state[\"image_query\"]\n",
    "    try:\n",
    "        image_url = DallEAPIWrapper(\n",
    "            model=\"dall-e-3\", size=\"1792x1024\", quality=\"standard\"\n",
    "        ).run(image_query)\n",
    "        urllib.request.urlretrieve(image_url, png_image)\n",
    "        with Image.open(png_image) as image:\n",
    "            image.save(jpg_image, optimized=True, quality=20)\n",
    "        os.remove(png_image)\n",
    "        response = post_file(jpg_image)\n",
    "        response = response.json()\n",
    "        image_ID=int(response.get(\"id\"))\n",
    "        image_url=response.get(\"guid\").get(\"rendered\")\n",
    "        response_en = post_file_en(jpg_image)\n",
    "        response_en = response_en.json()\n",
    "        image_ID_en=int(response_en.get(\"id\"))\n",
    "        image_url_en=response_en.get(\"guid\").get(\"rendered\")\n",
    "        return {\n",
    "            \"image_ID\": image_ID,\n",
    "            \"image_url\": image_url,\n",
    "            \"image_ID_en\": image_ID_en,\n",
    "            \"image_url_en\": image_url_en,\n",
    "        }\n",
    "    except:\n",
    "        return {\"image_ID\": -1, \"image_url\": \"\",\"image_ID_en\": -1, \"image_url_en\": \"\"}\n",
    "\n",
    "\n",
    "def publish_post(state):\n",
    "    title = state[\"title\"]\n",
    "    image_ID = state[\"image_ID\"]\n",
    "    image_url = state[\"image_url\"]\n",
    "    tags = state[\"tags\"]\n",
    "    reference = state[\"reference\"]\n",
    "    if image_ID == -1:\n",
    "        image_url = \"https://www.forwardpathway.com/wp-content/uploads/2024/06/fp_college_news_default.jpg\"\n",
    "        image_ID = 107009\n",
    "    content = state[\"content\"]\n",
    "    content = content + \"\"\"\\n### 参考新闻资料:\"\"\"\n",
    "    for key in reference:\n",
    "        content = content + \"\"\"\\n1. [{}]({})\"\"\".format(reference[key], key)\n",
    "    content = markdown.markdown(\n",
    "        content.replace(\"[image_placeholder]\", \"\"\"<img src=\"{}\">\"\"\".format(image_url)),extensions=['tables','footnotes']\n",
    "    )\n",
    "    (content, new_tags) = insert_keyword_url(content)\n",
    "    tags = tags | new_tags\n",
    "    post_post(\n",
    "        title, content, post_status=\"publish\", featured_media_id=image_ID, tags=tags\n",
    "    )\n",
    "    return\n",
    "\n",
    "def publish_post_en(state):\n",
    "    title = state[\"title_en\"]\n",
    "    image_ID = state[\"image_ID_en\"]\n",
    "    image_url = state[\"image_url_en\"]\n",
    "    tags = state[\"tags_en\"]\n",
    "    reference = state[\"reference\"]\n",
    "    if image_ID == -1:\n",
    "        image_url = \"https://www.forwardpathway.us/wp-content/uploads/2024/07/fp_college_news_default.jpg\"\n",
    "        image_ID = 15899\n",
    "    content = state[\"content_en\"]\n",
    "    content = content + \"\"\"\\n### News References:\"\"\"\n",
    "    for key in reference:\n",
    "        content = content + \"\"\"\\n1. [{}]({})\"\"\".format(reference[key], key)\n",
    "    content = markdown.markdown(\n",
    "        content.replace(\"[image_placeholder]\", \"\"\"<img src=\"{}\">\"\"\".format(image_url)),extensions=['tables','footnotes']\n",
    "    )\n",
    "    (content, new_tags) = insert_keyword_url(content,type='en')\n",
    "    #tags = tags | new_tags\n",
    "    post_post_en(\n",
    "        title, content, post_status=\"publish\", featured_media_id=image_ID, tags=tags\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d97d8c7-e31a-4d3f-9914-1b44d7b250e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Build LangGraph ####################################\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"summary_node\", summary)\n",
    "workflow.add_node(\"more_topics\", more_topics)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"rewrite\", rewrite)\n",
    "workflow.add_node(\"rewrite_en\", rewrite_en)\n",
    "workflow.add_node(\"article_metas\", article_metas)\n",
    "# workflow.add_node(\"format_article\", format_article)\n",
    "workflow.add_node(\"generate_image\", generate_image)\n",
    "workflow.add_node(\"publish_post\", publish_post)\n",
    "workflow.add_node(\"publish_post_en\", publish_post_en)\n",
    "\n",
    "workflow.set_entry_point(\"summary_node\")\n",
    "workflow.add_edge(\"summary_node\", \"more_topics\")\n",
    "workflow.add_edge(\"more_topics\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\", \"rewrite\")\n",
    "workflow.add_edge(\"rewrite\", \"rewrite_en\")\n",
    "workflow.add_edge(\"rewrite_en\", \"article_metas\")\n",
    "workflow.add_edge(\"article_metas\", \"generate_image\")\n",
    "workflow.add_edge(\"generate_image\", \"publish_post\")\n",
    "workflow.add_edge(\"publish_post\", \"publish_post_en\")\n",
    "workflow.add_edge(\"publish_post_en\", END)\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e480956-4fc3-4472-95a6-471f83661a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "# display(Image(app.get_graph().draw_mermaid_png(),width=300))\n",
    "# img=app.get_graph().draw_mermaid_png()\n",
    "# with open('GraphFlow.png','wb') as png:\n",
    "#    png.write(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d527dae-576f-4c75-98e8-17f917e0dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls():\n",
    "    connection = pymysql.connect(\n",
    "        db=os.environ[\"db_name\"],\n",
    "        user=os.environ[\"db_user\"],\n",
    "        passwd=os.environ[\"db_pass\"],\n",
    "        host=os.environ[\"db_host\"],\n",
    "        port=3306,\n",
    "        cursorclass=pymysql.cursors.DictCursor,\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = \"SELECT url FROM fp_chatGPT.news_urls WHERE post IS NULL OR post = 0 ORDER BY RAND() LIMIT 1\"\n",
    "    rows_count = cursor.execute(query)\n",
    "    if rows_count > 0:\n",
    "        rows = cursor.fetchall()\n",
    "        urls = [row[\"url\"] for row in rows]\n",
    "    else:\n",
    "        urls = []\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bb88859-ae79-4046-abd8-5e0a37266760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_url_flag(url=\"\"):\n",
    "    if len(url) > 0:\n",
    "        connection = pymysql.connect(\n",
    "            db=os.environ[\"db_name\"],\n",
    "            user=os.environ[\"db_user\"],\n",
    "            passwd=os.environ[\"db_pass\"],\n",
    "            host=os.environ[\"db_host\"],\n",
    "            port=3306,\n",
    "            cursorclass=pymysql.cursors.DictCursor,\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        query = \"UPDATE fp_chatGPT.news_urls SET post=1 WHERE url=%s\"\n",
    "        cursor.execute(query, url)\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64236d7d-b439-46b0-b352-71e34f2e6b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Initial Summary:  https://www.nyu.edu/about/news-publications/news/2024/july/researchers-find-depression-assessments-reliable-across-technolo.html\n",
      "------ Topics Summary:  https://www.nature.com/articles/s41746-024-01100-0\n",
      "------ Topics Summary:  https://www.nature.com/articles/s41591-024-02864-4\n",
      "------ Topics Summary:  https://www.nature.com/articles/s44184-023-00040-z\n",
      "------ Topics Summary:  https://publichealth.berkeley.edu/news-media/school-news/lonnie-snowden-receives-funding-to-study-mental-health-care-disparities\n",
      "1 finished\n"
     ]
    }
   ],
   "source": [
    "urls = get_urls()\n",
    "n_url = 0\n",
    "for url in urls:\n",
    "    try:\n",
    "        app.invoke({\"url\": url})\n",
    "        set_url_flag(url)\n",
    "        n_url = n_url + 1\n",
    "        print(n_url, \"finished\")\n",
    "    except Exception as e:\n",
    "        print(\"error for url: \", url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b62030-d7cc-4250-b9c1-41041d802652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
